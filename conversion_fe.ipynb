{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convmixer-conversion-fe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM9GVGGr7wT9T9atMIfwPwy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishit-dagli/ConvMixer-torch2tf/blob/main/conversion_fe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WABtusco3WjU"
      },
      "source": [
        "# Conversion Scripts\n",
        "\n",
        "Authors: [Rishit Dagli](https://twitter.com/rishit_dagli)\n",
        "\n",
        "In this Notebook we convert these 3 ConvMixer models to TensorFlow SavedModels:\n",
        "\n",
        "| Model Name | resolution | acc@1 | #params | File Size | TensorFlow Model |\n",
        "|------------|:---:|:---:|:---:|----------:|:--------------:|\n",
        "| ConvMixer-1536/20 | 224x224 | 81.37 | 51.6 x 10^6 | 184MB | [github](https://github.com/Rishit-dagli/ConvMixer-torch2tf/releases/download/untagged-de81892a1b06347b8d97/convmixer_1536_20.tar.gz)/[drive](https://drive.google.com/file/d/1qrzap4vi2KFQTHxf9h_AMbWGvtbP5rIA/view?usp=sharing)/[bucket](https://storage.googleapis.com/convmixer-hubmodels.appspot.com/convmixer_1536_20.tar.gz) |\n",
        "| ConvMixer-768/32 | 224x224 | 80.16 | 21.1 x 10^6 | 75MB | [github](https://github.com/Rishit-dagli/ConvMixer-torch2tf/releases/download/untagged-de81892a1b06347b8d97/convmixer_768_32.tar.gz)/[drive](https://drive.google.com/file/d/1NJgHKjPd3YC8XHypQIs5A05XKd15o0s3/view?usp=sharing)/[bucket](https://storage.googleapis.com/convmixer-hubmodels.appspot.com/convmixer_768_32.tar.gz) |\n",
        "| ConvMixer-1024/20 | 224x224 | 76.94 | 24.4 x 10^6 | 87MB | [github](https://github.com/Rishit-dagli/ConvMixer-torch2tf/releases/download/untagged-de81892a1b06347b8d97/convmixer_1024_20.tar.gz)/[drive](https://drive.google.com/file/d/1--jRgK0KmLtWCJswYtfxSIfEcjAOrJyv/view?usp=sharing)/[bucket](https://storage.googleapis.com/convmixer-hubmodels.appspot.com/convmixer_1024_20.tar.gz) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbzxwe_EyHhF"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We start off by installing [`timm`](https://github.com/rwightman/pytorch-image-models) but notice we are installing from my fork of it, this fork includes code from `timm_convmixer.py` from this repo. This updates the way the model handles padding in a `nn.Conv2D` layer to allow it to be converted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T946qhB9wjJe",
        "outputId": "47d705ec-92b7-4f9e-e824-69029bc3eca3"
      },
      "source": [
        "!pip install git+https://github.com/Rishit-dagli/pytorch-image-models.git@Rishit-dagli-convmixer-tf#egg=timm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Cloning https://github.com/Rishit-dagli/pytorch-image-models.git (to revision Rishit-dagli-convmixer-tf) to /tmp/pip-install-1yjryj_r/timm_596a2d9d3b8b4daa9ba6fcd011b03f54\n",
            "  Running command git clone -q https://github.com/Rishit-dagli/pytorch-image-models.git /tmp/pip-install-1yjryj_r/timm_596a2d9d3b8b4daa9ba6fcd011b03f54\n",
            "  Running command git checkout -b Rishit-dagli-convmixer-tf --track origin/Rishit-dagli-convmixer-tf\n",
            "  Switched to a new branch 'Rishit-dagli-convmixer-tf'\n",
            "  Branch 'Rishit-dagli-convmixer-tf' set up to track remote branch 'Rishit-dagli-convmixer-tf' from 'origin'.\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Building wheels for collected packages: timm\n",
            "  Building wheel for timm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timm: filename=timm-0.5.0-py3-none-any.whl size=417714 sha256=a6412fcc4361a9700a33145b96893ff633b8ee89e9495190a7c6cffa223a44a0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hsmkuozg/wheels/29/a9/a4/4ad1e14f936318e78dec16e4541708836546145fafa44e5915\n",
            "Successfully built timm\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zf68VqOzBbW",
        "outputId": "9a8c8ebf-a055-46ad-b979-719caadbd1db"
      },
      "source": [
        "!pip install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --upgrade"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.11.0.dev20211026%2Bcpu-cp37-cp37m-linux_x86_64.whl (151.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 151.2 MB 34 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cpu/torchvision-0.12.0.dev20211025%2Bcpu-cp37-cp37m-linux_x86_64.whl (16.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.8 MB 93 kB/s \n",
            "\u001b[?25hCollecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cpu/torchaudio-0.11.0.dev20211025%2Bcpu-cp37-cp37m-linux_x86_64.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.11.0.dev20211025%2Bcpu-cp37-cp37m-linux_x86_64.whl (151.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 151.2 MB 23 kB/s \n",
            "\u001b[?25hInstalling collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu111\n",
            "    Uninstalling torch-1.9.0+cu111:\n",
            "      Successfully uninstalled torch-1.9.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu111\n",
            "    Uninstalling torchvision-0.10.0+cu111:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.11.0.dev20211025+cpu which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.11.0.dev20211025+cpu torchaudio-0.11.0.dev20211025+cpu torchvision-0.12.0.dev20211025+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsAJxZqg1Ra4",
        "outputId": "48e6e07e-230f-4420-8ee3-11f8648a19bb"
      },
      "source": [
        "!pip install onnx onnx-tf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.10.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.3 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting onnx-tf\n",
            "  Downloading onnx_tf-1.9.0-py3-none-any.whl (222 kB)\n",
            "\u001b[K     |████████████████████████████████| 222 kB 57.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.7.4.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.19.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from onnx) (1.15.0)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 57.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from onnx-tf) (3.13)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->onnx-tf) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons, onnx, onnx-tf\n",
            "Successfully installed onnx-1.10.1 onnx-tf-1.9.0 tensorflow-addons-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtx6V6Zz5jDN"
      },
      "source": [
        "import os\n",
        "from io import BytesIO\n",
        "\n",
        "import numpy as np\n",
        "import onnx\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import timm\n",
        "import torch\n",
        "from onnx_tf.backend import prepare\n",
        "from PIL import Image"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nqO5Bp6A7h-"
      },
      "source": [
        "## Choose pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAIg0mTSzmgI"
      },
      "source": [
        "# fmt: off\n",
        "\n",
        "#@title Choose a model type\n",
        "model_variant = \"convmixer_1024_20_ks9_p14\" #@param ['convmixer_1536_20', 'convmixer_768_32', 'convmixer_1024_20_ks9_p14']\n",
        "resolution = [224, 224]\n",
        "num_classes = 1000\n",
        "\n",
        "# fmt: on"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbBQ209bA-SH"
      },
      "source": [
        "## Loading PyTorch pre-trained Model\n",
        "\n",
        "We will now load the pre-trained PyTorch models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLuWhSGKkyzK",
        "outputId": "18ec2e03-065c-46a5-ce39-b4c7974815f5"
      },
      "source": [
        "model = timm.create_model(model_variant, pretrained=True, num_classes = 0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/tmp-iclr/convmixer/releases/download/timm-v1.0/convmixer_1024_20_ks9_p14.pth.tar\" to /root/.cache/torch/hub/checkpoints/convmixer_1024_20_ks9_p14.pth.tar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFwVkfn78bg1"
      },
      "source": [
        "### Transpose channel axes\n",
        "\n",
        "Often in PyTorch we use the channels first data format meaning you would want to pass in input $(N, C_{in}, H_{in}, W_{in})$ wwhereas in TensorFlow you would often want to use the channels last format; pass in input as $(N, H_{in}, W_{in}, C_{in})$. You can also notice that the default data format for [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) is channels first and for [tf.keras.layers.Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) is channels last.\n",
        "\n",
        "To do so we create a wrapper model over the earlier model we created which just transposes the channel axes in the input tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeSjQjYyR-Sc"
      },
      "source": [
        "class TransposeModelWrapper(torch.nn.Module):\n",
        "    def __init__(self, main_module):\n",
        "        super(TransposeModelWrapper, self).__init__()\n",
        "        self.main_module = main_module\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.transpose(x, 1, 3)\n",
        "        return self.main_module(x)\n",
        "\n",
        "\n",
        "transposed_model = TransposeModelWrapper(model)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IiB5zPMA3U8"
      },
      "source": [
        "## Convert to ONNX\n",
        "\n",
        "We set `dynamic_axes` to allow the converted model operate with arbitrary batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkK6os42l3W6"
      },
      "source": [
        "batch_size = 1\n",
        "x = torch.randn(batch_size, resolution[0], resolution[1], 3)\n",
        "model_onnx = model_variant + \".onnx\"\n",
        "\n",
        "torch.onnx.export(\n",
        "    transposed_model,\n",
        "    x,\n",
        "    model_onnx,\n",
        "    opset_version=9,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLSGDi0PBCee"
      },
      "source": [
        "## Create a TensorFlow SavedModel\n",
        "\n",
        "We will first load the onnx model and then check that the IR is well formed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvER3xjP0w7L"
      },
      "source": [
        "onnx_model = onnx.load(model_onnx)\n",
        "onnx.checker.check_model(onnx_model)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoomhUy8-uji"
      },
      "source": [
        "We will now use the ONNX TensorFlow backend to convert our model to a TensorFlow SavedModel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsD-0BMX4P8e"
      },
      "source": [
        "os.environ['onnx_model_dir'] = \"/content/\" + model_variant + \".onnx\"\n",
        "os.environ['saved_model_dir'] = \"/content/tf_\" + model_variant"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3SDlDUuqDlf",
        "outputId": "71f33a61-0cef-4f8c-dada-76972c8abc44"
      },
      "source": [
        "!onnx-tf convert -i $onnx_model_dir -o $saved_model_dir"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-10-26 08:26:25,653 - onnx-tf - INFO - Start converting onnx pb to tf saved model\n",
            "2021-10-26 08:26:26.062934: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-10-26 08:26:26.063007: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (d0285cb69f4f): /proc/driver/nvidia/version does not exist\n",
            "2021-10-26 08:26:38.571970: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
            "2021-10-26 08:32:58,850 - onnx-tf - INFO - Converting completes successfully.\n",
            "INFO:onnx-tf:Converting completes successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNL-EIFTBGK1"
      },
      "source": [
        "## Verify SavedModel works as expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-Ns6QFJ4-Qm"
      },
      "source": [
        "saved_model_dir = \"/content/tf_\" + model_variant"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_ETwInCBI5Q"
      },
      "source": [
        "### Load SavedModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkH4gI6yBYvE"
      },
      "source": [
        "model = tf.saved_model.load(saved_model_dir)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5HqqOiQAI66"
      },
      "source": [
        "layer = hub.KerasLayer(\n",
        "    saved_model_dir, signature=\"serving_default\", signature_outputs_as_dict=True\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj9K137JD4QL"
      },
      "source": [
        "### Check input TypeSpec\n",
        "\n",
        "We will now use the SavedModel CLI to verify that our SavedModel has the correct input type spec which should be $(-1, 224, 224, 3)$ depending on which model you converted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxrJpELSBz1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f170bc-4941-490e-c7bf-e7b650a8aa3a"
      },
      "source": [
        "!saved_model_cli show --dir $saved_model_dir --tag_set serve --signature_def serving_default"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The given SavedModel SignatureDef contains the following input(s):\n",
            "  inputs['input'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 224, 224, 3)\n",
            "      name: serving_default_input:0\n",
            "The given SavedModel SignatureDef contains the following output(s):\n",
            "  outputs['output'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, -1)\n",
            "      name: PartitionedCall:0\n",
            "Method name is: tensorflow/serving/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yh4fgJqKib-"
      },
      "source": [
        "## Inference with the model\n",
        "\n",
        "We will now try inferencing with our TensorFlow SavedModel on a dummy tensor.\n",
        "\n",
        "In this example we try on a dummy tensor with $N=5$ to verify using an arbitary batch size works well. The output of these code cells should be:\n",
        "\n",
        "- `TensorShape([5, 1024])`\n",
        "- `TensorShape([1, 1024])`\n",
        "\n",
        "depending on which model you convert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm4APC-XOSPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a10bea82-a1ba-4b51-a1be-7f6be009e5ca"
      },
      "source": [
        "dummy = model.signatures[\"serving_default\"](\n",
        "    tf.random.normal([5, resolution[0], resolution[1], 3])\n",
        ")\n",
        "list(dummy.values())[0].shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([5, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAtqFAl5SCp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7785fdfa-63be-4a1b-916d-4d181722bc7e"
      },
      "source": [
        "dummy = layer(tf.random.normal([1, resolution[0], resolution[1], 3]))\n",
        "list(dummy.values())[0].shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu7WGw-fiBAg"
      },
      "source": [
        "## References\n",
        "\n",
        "[1] Anonymous. Patches Are All You Need? 2021. openreview.net, https://openreview.net/forum?id=TVHS5Y4dNvM.\n",
        "\n",
        "[2] Official Code Implementation: https://github.com/tmp-iclr/convmixer\n",
        "\n",
        "[3] Ross Wightman, . \"PyTorch Image Models.\" https://github.com/rwightman/pytorch-image-models."
      ]
    }
  ]
}